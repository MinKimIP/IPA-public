{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinKimIP/IPA-public/blob/master/uspto-tm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wZ8dF5H5rz2",
        "colab_type": "text"
      },
      "source": [
        "# Obtain USPTO trademark data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c22ELaE_Yurz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install pyarrow\n",
        "!pip install pandas --upgrade --force\n",
        "!pip install xmltodict\n",
        "!pip install tqdm\n",
        "\n",
        "# restart kernel (or runtime) after installation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F_Q1S7oMsGX",
        "colab_type": "code",
        "outputId": "83485e09-67d6-4414-a1f7-3d61eb4f12d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import io\n",
        "import os\n",
        "import sys\n",
        "import requests\n",
        "from datetime import timedelta, date\n",
        "from zipfile import ZipFile\n",
        "import glob\n",
        "import json\n",
        "import xmltodict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "print(pd.__version__) # need version 1.0.0 or newer\n",
        "from pandas import json_normalize\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ySKLIjcP6rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# XML-read dataframe handlers\n",
        "\n",
        "def normalise(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    for column in df.columns:\n",
        "        try:\n",
        "            df = df.reset_index(drop=True)\n",
        "            df = df.join(json_normalize(df[column]).add_prefix(column)).drop(columns=[column])\n",
        "        except:\n",
        "            df = df\n",
        "    \n",
        "    df = df.pipe(clean_column_names)\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def explode(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    for column in df.columns:\n",
        "        try:\n",
        "            df = df.explode(column)\n",
        "            df = df.reset_index(drop=True)\n",
        "        except:\n",
        "            df = df\n",
        "    return df\n",
        "\n",
        "\n",
        "def select_or_create_columns(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
        "    df1 = df.copy()\n",
        "    before = df.columns\n",
        "    for column in columns:\n",
        "        if column not in before:\n",
        "            df1[column] = np.nan\n",
        "        else:\n",
        "            df1[column] = df[column]\n",
        "    \n",
        "    df1 = df1[columns]\n",
        "    \n",
        "    return df1\n",
        "\n",
        "\n",
        "def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df.columns = [x.replace('.', '').replace(':', '').replace('#', '').replace('@', '').replace('-', '_') for x in df.columns]\n",
        "    return df\n",
        "\n",
        "\n",
        "def drop_problematic_columns(df: pd.DataFrame, columns: str) -> pd.DataFrame:\n",
        "    for column in columns:\n",
        "        if column in df.columns:\n",
        "            df = df.drop(columns = column)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqAoz6BWCYEK",
        "colab_type": "text"
      },
      "source": [
        "# Get Historical Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaoTBltwOOLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a list of historical zip files\n",
        "\n",
        "link_base = 'https://bulkdata.uspto.gov/data/trademark/dailyxml/applications/'\n",
        "zip_name_base = 'apc18840407-20191231'\n",
        "\n",
        "zip_files = []\n",
        "\n",
        "for i in range(1, 66):\n",
        "  zip_files.append(f\"{link_base}{zip_name_base}-{str(i).rjust(2, '0')}.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r9uEoUSg-2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Might take a long time. So select a batch you want to process in this session\n",
        "\n",
        "zip_files_batch = zip_files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1mrsRx3DIuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_like_yyyymmdd(date_string: str) -> bool:\n",
        "  if len(date_string) != 8:\n",
        "    return False\n",
        "  else:\n",
        "    yyyy = int(date_string[:4])\n",
        "    mm = int(date_string[4:6])\n",
        "    dd = int(date_string[6:])\n",
        "\n",
        "    valid_year = 1840 < yyyy < 2050\n",
        "    valid_month = 1 <= mm <= 12\n",
        "    valid_day = 1 <= dd <= 31\n",
        "\n",
        "    return valid_year & valid_month & valid_day\n",
        "\n",
        "\n",
        "def column_exists(df: pd.DataFrame, column: str) -> bool:\n",
        "  try:\n",
        "    df[column]\n",
        "    return True\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "\n",
        "for zip_file in zip_files_batch:\n",
        "  zip_name = os.path.basename(zip_file).replace('.zip', '')\n",
        "  print(zip_name)\n",
        "  r = requests.get(zip_file)\n",
        "  with ZipFile(io.BytesIO(r.content)) as zf:\n",
        "    xml_files = [x for x in zf.namelist() if x.endswith('.xml')]\n",
        "    for xml_file in xml_files:\n",
        "      with zf.open(xml_file) as xf:\n",
        "        xml = xf.read()\n",
        "        xmldict = xmltodict.parse(xml)\n",
        "        root = xmldict['trademark-applications-daily']['application-information']['file-segments']['action-keys']\n",
        "        del xmldict\n",
        "        df = pd.DataFrame.from_dict(root, orient='index').T\n",
        "        del root\n",
        "        df = df.pipe(explode).pipe(normalise).pipe(clean_column_names)\n",
        "        df.columns = [(x.replace('case_file_header', '')\n",
        "                        .replace('case_filecase_file_event_statements', '')\n",
        "                        .replace('case_filecase_file_statements', '')\n",
        "                        .replace('case_fileclassifications', '')\n",
        "                        .replace('case_filecase_file_owners', '')\n",
        "                        .replace('case_file_', '')\n",
        "                        .replace('case_file', '')\n",
        "                        .replace('madrid_international_filing_requestsmadrid_international_filing_recordmadrid_history_events', '')\n",
        "                        .replace('madrid_international_filing_requestsmadrid_international_filing_record', '')\n",
        "                        .replace('foreign_applications', '')\n",
        "                        .replace('prior_registration_applications', '')\n",
        "                        .replace('event_statement', 'event_statement_')\n",
        "                        .replace('classification', 'classification_')\n",
        "                        .replace('correspondent', 'correspondent_'))\n",
        "                      for x in df.columns]\n",
        "        \n",
        "        application_columns = ['action_key',\n",
        "                               'serial_number',\n",
        "                               'registration_number',\n",
        "                               'transaction_date',\n",
        "                               'filing_date',\n",
        "                               'registration_date',\n",
        "                               'abandonment_date',\n",
        "                               'status_code',\n",
        "                               'status_date',\n",
        "                               'mark_identification',\n",
        "                               'statementtext',\n",
        "                               'mark_drawing_code',\n",
        "                               'trademark_in',\n",
        "                               'collective_trademark_in',\n",
        "                               'service_mark_in',\n",
        "                               'collective_service_mark_in',\n",
        "                               'collective_membership_mark_in',\n",
        "                               'certification_mark_in',\n",
        "                               'cancellation_pending_in',\n",
        "                               'concurrent_use_in',\n",
        "                               'foreign_priority_in',\n",
        "                               'change_registration_in',\n",
        "                               'intent_to_use_in',\n",
        "                               'intent_to_use_current_in',\n",
        "                               'filed_as_use_application_in',\n",
        "                               'international_registration_number',\n",
        "                               'international_registration_date']\n",
        "\n",
        "        subset = ['registration_number',\n",
        "                  'transaction_date',\n",
        "                  'filing_date',\n",
        "                  'registration_date',\n",
        "                  'abandonment_date',\n",
        "                  'status_code',\n",
        "                  'status_date',\n",
        "                  'mark_identification',\n",
        "                  'statementtext',\n",
        "                  'mark_drawing_code',\n",
        "                  'trademark_in',\n",
        "                  'collective_trademark_in',\n",
        "                  'service_mark_in',\n",
        "                  'collective_service_mark_in',\n",
        "                  'collective_membership_mark_in',\n",
        "                  'certification_mark_in',\n",
        "                  'cancellation_pending_in',\n",
        "                  'concurrent_use_in',\n",
        "                  'foreign_priority_in',\n",
        "                  'change_registration_in',\n",
        "                  'intent_to_use_in',\n",
        "                  'intent_to_use_current_in',\n",
        "                  'filed_as_use_application_in',\n",
        "                  'international_registration_number',\n",
        "                  'international_registration_date']\n",
        "\n",
        "        if column_exists(df, ''):\n",
        "          application_part_a = (df.copy()\n",
        "                                .loc[df[''].isna(),:]\n",
        "                                .pipe(select_or_create_columns, application_columns)\n",
        "                                .drop_duplicates())\n",
        "          \n",
        "          application_part_b = (df.copy()\n",
        "                                .loc[df[''].notna(), ['action_key', 'serial_number', '']]\n",
        "                                .pipe(explode).pipe(normalise)\n",
        "                                .pipe(select_or_create_columns, application_columns)\n",
        "                                .dropna(subset=subset, how='all')\n",
        "                                .drop_duplicates())\n",
        "\n",
        "          application = pd.concat([application_part_a, application_part_b]).drop_duplicates()\n",
        "          del application_part_a\n",
        "          del application_part_b\n",
        "        else:\n",
        "          application = (df.copy()\n",
        "                           .pipe(select_or_create_columns, application_columns)\n",
        "                           .drop_duplicates())\n",
        "        \n",
        "        for column in application.columns:\n",
        "          if 'date' in column:\n",
        "            is_valid_date = application[column].astype(str).apply(is_like_yyyymmdd)\n",
        "            application.loc[~is_valid_date, column] = np.nan\n",
        "            application[column] = pd.to_datetime(application[column], format = '%Y%m%d')\n",
        "          elif column.endswith('_in'):\n",
        "            application[column] = application[column].fillna(False).replace('F', False).replace('T', True)\n",
        "\n",
        "        application.to_parquet(f'application-{zip_name}.parquet', index=False)\n",
        "        del application\n",
        "\n",
        "        classification = (df.copy()\n",
        "                            .pipe(select_or_create_columns, ['action_key',\n",
        "                                                             'serial_number',\n",
        "                                                             'classification_international_code'])\n",
        "                            .pipe(explode)\n",
        "                            .drop_duplicates())\n",
        "\n",
        "        classification.to_parquet(f'classification-{zip_name}.parquet', index=False)\n",
        "        del classification\n",
        "\n",
        "        owner_columns = ['action_key',\n",
        "                         'serial_number',\n",
        "                         'ownerparty_name',\n",
        "                         'ownercity',\n",
        "                         'ownerstate',\n",
        "                         'ownerpostcode',\n",
        "                         'ownernationalitystate',\n",
        "                         'ownernationalitycountry',\n",
        "                         'ownerentity_statement',\n",
        "                         'ownercountry']\n",
        "\n",
        "        if column_exists(df, 'owner'):\n",
        "          owner_part_a = (df.copy()\n",
        "                          .loc[df['owner'].notna(), ['action_key', 'serial_number', 'owner']]\n",
        "                          .pipe(explode).pipe(normalise)\n",
        "                          .pipe(select_or_create_columns, owner_columns))\n",
        "\n",
        "          owner_part_b = (df.copy()\n",
        "                          .loc[df['owner'].isna(),:]\n",
        "                          .pipe(select_or_create_columns, owner_columns))\n",
        "\n",
        "          owner = pd.concat([owner_part_a, owner_part_b]).drop_duplicates()\n",
        "          del owner_part_a\n",
        "          del owner_part_b\n",
        "        else:\n",
        "          owner = (df.copy()\n",
        "                    .pipe(select_or_create_columns, owner_columns))\n",
        "        \n",
        "        owner.to_parquet(f'owner-{zip_name}.parquet', index=False)\n",
        "        del owner\n",
        "\n",
        "        del df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hretF1fWGb57",
        "colab_type": "text"
      },
      "source": [
        "# Get Daily Update Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGNqgb6CGakl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a list of daily zip files\n",
        "\n",
        "def daterange(start_date, end_date):\n",
        "    for n in range(int ((end_date - start_date).days)):\n",
        "        yield start_date + timedelta(n)\n",
        "\n",
        "start_date = date(2020, 1, 1)\n",
        "end_date = date(2020, 3, 24)\n",
        "\n",
        "link_base = 'https://bulkdata.uspto.gov/data/trademark/dailyxml/applications/'\n",
        "zip_name_base = 'apc'\n",
        "\n",
        "zip_files = []\n",
        "\n",
        "for single_date in daterange(start_date, end_date):\n",
        "    zip_files.append(link_base + zip_name_base + single_date.strftime(\"%y%m%d\") + '.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq5DYsvoGapk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "7a62b555-e543-4cbc-fd29-25623d1fc5f2"
      },
      "source": [
        "def is_like_yyyymmdd(date_string: str) -> bool:\n",
        "  if len(date_string) != 8:\n",
        "    return False\n",
        "  else:\n",
        "    yyyy = int(date_string[:4])\n",
        "    mm = int(date_string[4:6])\n",
        "    dd = int(date_string[6:])\n",
        "\n",
        "    valid_year = 1840 < yyyy < 2050\n",
        "    valid_month = 1 <= mm <= 12\n",
        "    valid_day = 1 <= dd <= 31\n",
        "\n",
        "    return valid_year & valid_month & valid_day\n",
        "\n",
        "\n",
        "def column_exists(df: pd.DataFrame, column: str) -> bool:\n",
        "  try:\n",
        "    df[column]\n",
        "    return True\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "\n",
        "for zip_file in zip_files:\n",
        "  zip_name = os.path.basename(zip_file).replace('.zip', '')\n",
        "  print(zip_name)\n",
        "  r = requests.get(zip_file)\n",
        "  with ZipFile(io.BytesIO(r.content)) as zf:\n",
        "    xml_files = [x for x in zf.namelist() if x.endswith('.xml')]\n",
        "    for xml_file in xml_files:\n",
        "      with zf.open(xml_file) as xf:\n",
        "        xml = xf.read()\n",
        "        xmldict = xmltodict.parse(xml)\n",
        "        root = {}\n",
        "        root['data'] = xmldict['trademark-applications-daily']['application-information']['file-segments']['action-keys']\n",
        "        del xmldict\n",
        "        df = pd.DataFrame.from_dict(root, orient='index').T\n",
        "        del root\n",
        "        df = df.pipe(explode).pipe(normalise).pipe(explode).pipe(normalise).pipe(clean_column_names)\n",
        "        df.columns = [(x.replace('case_file_header', '')\n",
        "                        .replace('case_filecase_file_event_statements', '')\n",
        "                        .replace('case_filecase_file_statements', '')\n",
        "                        .replace('case_fileclassifications', '')\n",
        "                        .replace('case_filecase_file_owners', '')\n",
        "                        .replace('case_file_', '')\n",
        "                        .replace('case_file', '')\n",
        "                        .replace('madrid_international_filing_requestsmadrid_international_filing_recordmadrid_history_events', '')\n",
        "                        .replace('madrid_international_filing_requestsmadrid_international_filing_record', '')\n",
        "                        .replace('foreign_applications', '')\n",
        "                        .replace('prior_registration_applications', '')\n",
        "                        .replace('event_statement', 'event_statement_')\n",
        "                        .replace('classification', 'classification_')\n",
        "                        .replace('correspondent', 'correspondent_'))\n",
        "                      for x in df.columns]\n",
        "        \n",
        "        application_columns = ['action_key',\n",
        "                               'serial_number',\n",
        "                               'registration_number',\n",
        "                               'transaction_date',\n",
        "                               'filing_date',\n",
        "                               'registration_date',\n",
        "                               'abandonment_date',\n",
        "                               'status_code',\n",
        "                               'status_date',\n",
        "                               'mark_identification',\n",
        "                               'statementtext',\n",
        "                               'mark_drawing_code',\n",
        "                               'trademark_in',\n",
        "                               'collective_trademark_in',\n",
        "                               'service_mark_in',\n",
        "                               'collective_service_mark_in',\n",
        "                               'collective_membership_mark_in',\n",
        "                               'certification_mark_in',\n",
        "                               'cancellation_pending_in',\n",
        "                               'concurrent_use_in',\n",
        "                               'foreign_priority_in',\n",
        "                               'change_registration_in',\n",
        "                               'intent_to_use_in',\n",
        "                               'intent_to_use_current_in',\n",
        "                               'filed_as_use_application_in',\n",
        "                               'international_registration_number',\n",
        "                               'international_registration_date']\n",
        "\n",
        "        subset = ['registration_number',\n",
        "                  'transaction_date',\n",
        "                  'filing_date',\n",
        "                  'registration_date',\n",
        "                  'abandonment_date',\n",
        "                  'status_code',\n",
        "                  'status_date',\n",
        "                  'mark_identification',\n",
        "                  'statementtext',\n",
        "                  'mark_drawing_code',\n",
        "                  'trademark_in',\n",
        "                  'collective_trademark_in',\n",
        "                  'service_mark_in',\n",
        "                  'collective_service_mark_in',\n",
        "                  'collective_membership_mark_in',\n",
        "                  'certification_mark_in',\n",
        "                  'cancellation_pending_in',\n",
        "                  'concurrent_use_in',\n",
        "                  'foreign_priority_in',\n",
        "                  'change_registration_in',\n",
        "                  'intent_to_use_in',\n",
        "                  'intent_to_use_current_in',\n",
        "                  'filed_as_use_application_in',\n",
        "                  'international_registration_number',\n",
        "                  'international_registration_date']\n",
        "\n",
        "        if column_exists(df, ''):\n",
        "          application_part_a = (df.copy()\n",
        "                                .loc[df[''].isna(),:]\n",
        "                                .pipe(select_or_create_columns, application_columns)\n",
        "                                .drop_duplicates())\n",
        "          \n",
        "          application_part_b = (df.copy()\n",
        "                                .loc[df[''].notna(), ['action_key', 'serial_number', '']]\n",
        "                                .pipe(explode).pipe(normalise)\n",
        "                                .pipe(select_or_create_columns, application_columns)\n",
        "                                .dropna(subset=subset, how='all')\n",
        "                                .drop_duplicates())\n",
        "\n",
        "          application = pd.concat([application_part_a, application_part_b]).drop_duplicates()\n",
        "          del application_part_a\n",
        "          del application_part_b\n",
        "        else:\n",
        "          application = (df.copy()\n",
        "                           .pipe(select_or_create_columns, application_columns)\n",
        "                           .drop_duplicates())\n",
        "        \n",
        "        for column in application.columns:\n",
        "          if 'date' in column:\n",
        "            is_valid_date = application[column].astype(str).apply(is_like_yyyymmdd)\n",
        "            application.loc[~is_valid_date, column] = np.nan\n",
        "            application[column] = pd.to_datetime(application[column], format = '%Y%m%d')\n",
        "          elif column.endswith('_in'):\n",
        "            application[column] = application[column].fillna(False).replace('F', False).replace('T', True)\n",
        "\n",
        "        application.to_parquet(f'application-{zip_name}.parquet', index=False)\n",
        "        del application\n",
        "\n",
        "        classification = (df.copy()\n",
        "                            .pipe(select_or_create_columns, ['action_key',\n",
        "                                                             'serial_number',\n",
        "                                                             'classification_international_code'])\n",
        "                            .pipe(explode)\n",
        "                            .drop_duplicates())\n",
        "\n",
        "        classification.to_parquet(f'classification-{zip_name}.parquet', index=False)\n",
        "        del classification\n",
        "\n",
        "        owner_columns = ['action_key',\n",
        "                         'serial_number',\n",
        "                         'ownerparty_name',\n",
        "                         'ownercity',\n",
        "                         'ownerstate',\n",
        "                         'ownerpostcode',\n",
        "                         'ownernationalitystate',\n",
        "                         'ownernationalitycountry',\n",
        "                         'ownerentity_statement',\n",
        "                         'ownercountry']\n",
        "\n",
        "        if column_exists(df, 'owner'):\n",
        "          owner_part_a = (df.copy()\n",
        "                          .loc[df['owner'].notna(), ['action_key', 'serial_number', 'owner']]\n",
        "                          .pipe(explode).pipe(normalise)\n",
        "                          .pipe(select_or_create_columns, owner_columns))\n",
        "\n",
        "          owner_part_b = (df.copy()\n",
        "                          .loc[df['owner'].isna(),:]\n",
        "                          .pipe(select_or_create_columns, owner_columns))\n",
        "\n",
        "          owner = pd.concat([owner_part_a, owner_part_b]).drop_duplicates()\n",
        "          del owner_part_a\n",
        "          del owner_part_b\n",
        "        else:\n",
        "          owner = (df.copy()\n",
        "                    .pipe(select_or_create_columns, owner_columns))\n",
        "        \n",
        "        owner.to_parquet(f'owner-{zip_name}.parquet', index=False)\n",
        "        del owner\n",
        "\n",
        "        del df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "apc200101\n",
            "apc200102\n",
            "apc200103\n",
            "apc200104\n",
            "apc200105\n",
            "apc200106\n",
            "apc200107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJItfXxUGaoB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "368300c1-a465-42cf-80d8-826f0e1207aa"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm-9vyzvmtQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}